package gss.ETLCode.bin;

public class Python_Airflow {


	public static String getDW(String tableName, String txtFileName, String sourceFileIsZip) {

		String rs = "#!/usr/bin/env python3\n"
				+ "# -*- coding: utf-8 -*-\n"
				+ "\n"
				+ "import datetime\n"
				+ "import pendulum\n"
				+ "from airflow.models import DAG\n"
				+ "from airflow.models import Variable\n"
				+ "from airflow.operators.bash import BashOperator\n"
				+ "from airflow.operators.dummy_operator import DummyOperator\n"
				+ "from airflow.operators.python_operator import PythonOperator\n"
				+ "#用在只要RETURN FALSE後面就skip\n"
				+ "from airflow.operators.python_operator import ShortCircuitOperator\n"
				+ "#用在根據條件看要跑哪條路\n"
				+ "from airflow.operators.python_operator import BranchPythonOperator\n"
				+ "#用在若有兩個前置其中一成功就要啟動去跑，此範例主要搭配BranchPythonOperator在用\n"
				+ "from airflow.utils.trigger_rule import TriggerRule\n"
				+ "from airflow.providers.microsoft.mssql.operators.mssql import MsSqlOperator\n"
				+ "from airflow.utils.dates import days_ago\n"
				+ "#用在故意讓task失敗\n"
				+ "from airflow.exceptions import AirflowException\n"
				+ "#用在判斷file是否存在\n"
				+ "from airflow.sensors.bash import BashSensor\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "# default params\n"
				+ "PROG_PATH=Variable.get(\"PROG_PATH\")\n"
				+ "DAG_WORKSPACE=Variable.get(\"DAG_WORKSPACE\")\n"
				+ "HADOOP_UPLOAD=Variable.get(\"HADOOP_UPLOAD\")\n"
				+ "download_folder=Variable.get(\"DW_WORK\")+ \"download/\"\n"
				+ "target_table = \"" + tableName + "\"\n"
				+ "exists_pattern=\""+txtFileName.substring(0,txtFileName.lastIndexOf("."))+"*.*\"\n"
				+ "pattern=\""+txtFileName.substring(0,txtFileName.lastIndexOf("."))+"*.*\"\n"
				+ "encryption=\"" + ("Y".equalsIgnoreCase(sourceFileIsZip) ? "-u" : "") + "\"\n"
				+ "download_path_exists=download_folder + exists_pattern\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "current_datetime=datetime.datetime.now()\n"
				+ "BATCHID=current_datetime.strftime(\"%Y%m%d%H%M%S\")\n"
				+ "SYS_DATE=BATCHID[0:8]\n"
				+ "\n"
				+ "SP_PAR=\"'A','','','\"+target_table+\"';\"\n"
				+ "query_start='''{{ dag_run.conf['query_start'] if dag_run.conf else \"exec spGetETLPrecheck '''+SP_PAR+'''\" }}'''\n"
				+ "query_end=\"exec dbo.spSetETLFinish '\" + target_table + \"';\"\n"
				+ "query_fail=\"exec dbo.spDelRunNow '\" + target_table + \"';\"\n"
				+ "\n"
				+ "def get_xcoms(**kwargs):\n"
				+ "#TABLENM|RC|YMDS|YMDE|YMS|YME|YS|YE|ERR_MSG\n"
				+ "    ti = kwargs['ti']\n"
				+ "    x = str(ti.xcom_pull(task_ids='check_status'))[3:-3].split(\"|\")\n"
				+ "    ti.xcom_push(key='TABLENM',value=x[0])\n"
				+ "    ti.xcom_push(key='RC',value=x[1])\n"
				+ "    ti.xcom_push(key='YMDS',value=x[2])\n"
				+ "    ti.xcom_push(key='YMDE',value=x[3])\n"
				+ "    ti.xcom_push(key='YMS',value=x[4])\n"
				+ "    ti.xcom_push(key='YME',value=x[5])\n"
				+ "    ti.xcom_push(key='YS',value=x[6])\n"
				+ "    ti.xcom_push(key='YE',value=x[7])\n"
				+ "    ti.xcom_push(key='ERR_MSG',value=x[8])\n"
				+ "    ti.xcom_push(key='BATCHID',value=BATCHID)\n"
				+ "    ti.xcom_push(key='SYS_DATE',value=SYS_DATE)\n"
				+ "\n"
				+ "# \n"
				+ "def condition_to_go(**kwargs):\n"
				+ "    ti = kwargs['ti']\n"
				+ "    x = str(ti.xcom_pull(task_ids='get_xcoms',key='RC'))\n"
				+ "    if x == \"0\":\n"
				+ "        return True\n"
				+ "    else:\n"
				+ "        raise AirflowException(\"prechk not success\")\n"
				+ "        return False\n"
				+ "\n"
				+ "default_args = {\n"
				+ "    'owner': 'post1',\n"
				+ "    'depends_on_past': False,\n"
				+ "    'email': ['bdbu_post@gss.com.tw'],\n"
				+ "    'email_on_failure': False,\n"
				+ "    'email_on_retry': False,\n"
				+ "    'retries': 0,\n"
				+ "    'retry_delay': datetime.timedelta(minutes=5),\n"
				+ "}\n"
				+ "\n"
				+ "\n"
				+ "dag = DAG(\n"
				+ "    dag_id='" + tableName + "',\n"
				+ "    default_args=default_args,\n"
				+ "    description='" + tableName + "',\n"
				+ "    catchup=False,\n"
				+ "    schedule_interval=None,\n"
				+ "    start_date=pendulum.datetime(2024, 1, 1, tz=\"Asia/Taipei\"),\n"
				+ "    tags=['etl'],\n"
				+ ")\n"
				+ "   \n"
				+ "\n"
				+ "check_status=MsSqlOperator(\n"
				+ "    task_id='check_status',\n"
				+ "    mssql_conn_id='MSAPPS',\n"
				+ "    sql=query_start,\n"
				+ "    dag=dag,\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "get_xcoms=PythonOperator(\n"
				+ "    task_id='get_xcoms',\n"
				+ "    python_callable=get_xcoms,\n"
				+ "    dag=dag,\n"
				+ ")\n"
				+ "\n"
				+ "condition_to_go = PythonOperator(\n"
				+ "    task_id='condition_to_go',\n"
				+ "    python_callable=condition_to_go,\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "check_file = BashSensor(\n"
				+ "    task_id='check_file',\n"
				+ "    bash_command=f'test -e {download_path_exists}',\n"
				+ "    mode='reschedule',\n"
				+ "    retry_delay=datetime.timedelta(minutes=1),\n"
				+ "    timeout=datetime.timedelta(minutes=3),\n"
				+ "    dag=dag,\n"
				+ ")\n"
				+ "\n"
				+ "get_file = BashOperator(\n"
				+ "    task_id='get_file',\n"
				+ "    bash_command=f'sh ~/script/get_file.sh -T {target_table} -F {pattern} {encryption}',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "cre_hadoop_folder = BashOperator(\n"
				+ "    task_id='cre_hadoop_folder',\n"
				+ "   bash_command=f\"sh {DAG_WORKSPACE}scripts/gssShell.sh fs -mkdir -p {HADOOP_UPLOAD}{target_table}/\",\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "put_file_2hdfs = BashOperator(\n"
				+ "    task_id='put_file_2hdfs',\n"
				+ "   bash_command=f\"sh {DAG_WORKSPACE}scripts/gssShell.sh fs -put {download_folder}{target_table}/WORK/{pattern} {HADOOP_UPLOAD}{target_table}/\",\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "BEFORE_C01 = BashOperator(\n"
				+ "    task_id='BEFORE_C01',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/BEFORE_C01.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "ODS_C01_UploadFile = BashOperator(\n"
				+ "    task_id='ODS_C01_UploadFile',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/ODS_C01_UploadFile.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "ODS_C02_GetDataFileName = BashOperator(\n"
				+ "    task_id='ODS_C02_GetDataFileName',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/ODS_C02_GetDataFileName.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "ODS_C03_GetErrorData = BashOperator(\n"
				+ "    task_id='ODS_C03_GetErrorData',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/ODS_C03_GetErrorData.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "ODS_C04_GetData = BashOperator(\n"
				+ "    task_id='ODS_C04_GetData',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/ODS_C04_GetData.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "ODS_C05_GetDoneFile = BashOperator(\n"
				+ "    task_id='ODS_C05_GetDoneFile',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/ODS_C05_GetDoneFile.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "TRUNCATE_ODS = BashOperator(\n"
				+ "    task_id='TRUNCATE_ODS',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/TRUNCATE_ODS.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "ODS_L06_LoadODS = BashOperator(\n"
				+ "    task_id='ODS_L06_LoadODS',\n"
				+ "    bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/ODS_L06_LoadODS.hql',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "file_done = BashOperator(\n"
				+ "    task_id='file_done',\n"
				+ "   bash_command=f'sh ~/script/done_file.sh -T {target_table}',\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "# 確認參數名稱是否為ACT_YM(年月)\n"
				+ "BACKUP_DW = BashOperator(\n"
				+ "    task_id='BACKUP_DW',\n"
				+ "    bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/BACKUP_DW.hql --var BATCHID={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='BATCHID') }}}} --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "TRUNCATE_DW = BashOperator(\n"
				+ "    task_id='TRUNCATE_DW',\n"
				+ "    bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/TRUNCATE_DW.hql --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "DW_L07_LoadDW = BashOperator(\n"
				+ "    task_id='DW_L07_LoadDW',\n"
				+ "    bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/DW_L07_LoadDW.hql --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "DW_L08_LoadDW_TMP = BashOperator(\n"
				+ "    task_id='DW_L08_LoadDW_TMP',\n"
				+ "    bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/DW_L08_LoadDW_TMP.hql --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "    dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "check_fail=MsSqlOperator(\n"
				+ "    task_id='check_fail',\n"
				+ "    mssql_conn_id='MSAPPS',\n"
				+ "    trigger_rule='one_failed',\n"
				+ "    sql=query_fail,\n"
				+ "    dag=dag,\n"
				+ ") \n"
				+ "\n"
				+ "end_status=MsSqlOperator(\n"
				+ "    task_id='end_status',\n"
				+ "    mssql_conn_id='MSAPPS',\n"
				+ "    trigger_rule='all_success',\n"
				+ "    sql=query_end,\n"
				+ "    dag=dag,\n"
				+ ") \n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "######### 確認執行順序\n"
				+ "check_status >> get_xcoms >> condition_to_go \n"
				+ "condition_to_go >> check_file >> get_file >> cre_hadoop_folder >> put_file_2hdfs\n"
				+ "put_file_2hdfs >> BEFORE_C01 >> ODS_C01_UploadFile >> ODS_C02_GetDataFileName >> ODS_C03_GetErrorData \n"
				+ "ODS_C03_GetErrorData >> ODS_C04_GetData >> ODS_C05_GetDoneFile >> TRUNCATE_ODS >> ODS_L06_LoadODS\n"
				+ "ODS_L06_LoadODS >> file_done >> BACKUP_DW >> TRUNCATE_DW >> DW_L07_LoadDW >> DW_L08_LoadDW_TMP >> [check_fail, end_status]\n"
				+ "\n"
				+ "\n"
				+ "";
			
		return rs;
	}
	
	public static String getDM(String tableName, String txtFileName) {

		String rs = "#!/usr/bin/env python3\n"
				+ "# -*- coding: utf-8 -*-\n"
				+ "\n"
				+ "import datetime\n"
				+ "import pendulum\n"
				+ "from airflow.models import DAG\n"
				+ "from airflow.models import Variable\n"
				+ "from airflow.operators.bash import BashOperator\n"
				+ "from airflow.operators.dummy_operator import DummyOperator\n"
				+ "from airflow.operators.python_operator import PythonOperator\n"
				+ "#用在只要RETURN FALSE後面就skip\n"
				+ "from airflow.operators.python_operator import ShortCircuitOperator\n"
				+ "#用在根據條件看要跑哪條路\n"
				+ "from airflow.operators.python_operator import BranchPythonOperator\n"
				+ "#用在若有兩個前置其中一成功就要啟動去跑，此範例主要搭配BranchPythonOperator在用\n"
				+ "from airflow.utils.trigger_rule import TriggerRule\n"
				+ "from airflow.providers.microsoft.mssql.operators.mssql import MsSqlOperator\n"
				+ "from airflow.utils.dates import days_ago\n"
				+ "#用在故意讓task失敗\n"
				+ "from airflow.exceptions import AirflowException\n"
				+ "#用在判斷file是否存在\n"
				+ "from airflow.sensors.bash import BashSensor\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "# default params\n"
				+ "PROG_PATH=Variable.get(\"PROG_PATH\")\n"
				+ "DAG_WORKSPACE=Variable.get(\"DAG_WORKSPACE\")\n"
				+ "target_table = \"" + tableName + "\"\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "current_datetime=datetime.datetime.now()\n"
				+ "BATCHID=current_datetime.strftime(\"%Y%m%d%H%M%S\")\n"
				+ "SYS_DATE=BATCHID[0:8]\n"
				+ "\n"
				+ "\n"
				+ "SP_PAR=\"'A','','','\"+target_table+\"';\"\n"
				+ "query_start='''{{ dag_run.conf['query_start'] if dag_run.conf else \"exec spGetETLPrecheck '''+SP_PAR+'''\" }}'''\n"
				+ "query_end=\"exec dbo.spSetETLFinish '\" + target_table + \"';\"\n"
				+ "query_fail=\"exec dbo.spDelRunNow '\" + target_table + \"';\"\n"
				+ "\n"
				+ "def get_xcoms(**kwargs):\n"
				+ "\n"
				+ "#TABLENM|RC|YMDS|YMDE|YMS|YME|YS|YE|ERR_MSG\n"
				+ "	ti = kwargs['ti']\n"
				+ "	x = str(ti.xcom_pull(task_ids='check_status'))[3:-3].split(\"|\")\n"
				+ "	ti.xcom_push(key='TABLENM',value=x[0])\n"
				+ "	ti.xcom_push(key='RC',value=x[1])\n"
				+ "	ti.xcom_push(key='YMDS',value=x[2])\n"
				+ "	ti.xcom_push(key='YMDE',value=x[3])\n"
				+ "	ti.xcom_push(key='YMS',value=x[4])\n"
				+ "	ti.xcom_push(key='YME',value=x[5])\n"
				+ "	ti.xcom_push(key='YS',value=x[6])\n"
				+ "	ti.xcom_push(key='YE',value=x[7])\n"
				+ "	ti.xcom_push(key='ERR_MSG',value=x[8])\n"
				+ "	ti.xcom_push(key='BATCHID',value=BATCHID)\n"
				+ "	ti.xcom_push(key='SYS_DATE',value=SYS_DATE)\n"
				+ "\n"
				+ "# \n"
				+ "def condition_to_go(**kwargs):\n"
				+ "	ti = kwargs['ti']\n"
				+ "	x = str(ti.xcom_pull(task_ids='get_xcoms',key='RC'))\n"
				+ "	if x == \"0\":\n"
				+ "		return True\n"
				+ "	else:\n"
				+ "		raise AirflowException(\"prechk not success\")\n"
				+ "		return False\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "\n"
				+ "default_args = {\n"
				+ "	'owner': 'post1',\n"
				+ "	'depends_on_past': False,\n"
				+ "	'email': ['carol_chang@gss.com.tw'],\n"
				+ "	'email_on_failure': False,\n"
				+ "	'email_on_retry': False,\n"
				+ "	'retries': 0,\n"
				+ "	'retry_delay': datetime.timedelta(minutes=5),\n"
				+ "}\n"
				+ "\n"
				+ "\n"
				+ "dag = DAG(\n"
				+ "	dag_id='" + tableName + "',\n"
				+ "	default_args=default_args,\n"
				+ "	description='" + tableName + "',\n"
				+ "	catchup=False,\n"
				+ "	schedule_interval=None,\n"
				+ "	#schedule_interval=\"30 11 2 2 *\",\n"
				+ "	#start_date=days_ago(2),\n"
				+ "	start_date=pendulum.datetime(2024, 1, 1, tz=\"Asia/Taipei\"),\n"
				+ "	tags=['etl'],\n"
				+ ")\n"
				+ "\n"
				+ "check_status=MsSqlOperator(\n"
				+ "	task_id='check_status',\n"
				+ "	mssql_conn_id='MSAPPS',\n"
				+ "	sql=query_start,\n"
				+ "	dag=dag,\n"
				+ ")\n"
				+ "\n"
				+ "\n"
				+ "get_xcoms=PythonOperator(\n"
				+ "	task_id='get_xcoms',\n"
				+ "	python_callable=get_xcoms,\n"
				+ "	dag=dag,\n"
				+ ")\n"
				+ "\n"
				+ "condition_to_go = PythonOperator(\n"
				+ "	task_id='condition_to_go',\n"
				+ "	python_callable=condition_to_go,\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "BEFORE_C01 = BashOperator(\n"
				+ "	task_id='BEFORE_C01',\n"
				+ "	bash_command=f'sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/BEFORE_C01.hql',\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "####################################\n"
				+ "# T系列需再自行調整\n"
				+ "####################################\n"
				+ "DM_T01 = BashOperator(\n"
				+ "	task_id='DM_T01',\n"
				+ "	bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/DM_T01.hql --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "# 確認參數名稱是否為ACT_YM(年月)\n"
				+ "BACKUP_DM = BashOperator(\n"
				+ "	task_id='BACKUP_DM',\n"
				+ "	bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/BACKUP_DM.hql --var BATCHID={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='BATCHID') }}}} --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "TRUNCATE_DM = BashOperator(\n"
				+ "	task_id='TRUNCATE_DM',\n"
				+ "	bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/TRUNCATE_DM.hql --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "# L系列的編號要對應HQL檔名\n"
				+ "DM_L02_LoadDM = BashOperator(\n"
				+ "	task_id='DM_L02_LoadDM',\n"
				+ "	bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/DM_L02_LoadDM.hql --var BATCHID={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='BATCHID') }}}} --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "DM_L03_LoadDM_TMP = BashOperator(\n"
				+ "	task_id='DM_L03_LoadDM_TMP',\n"
				+ "	bash_command=f\"sh {DAG_WORKSPACE}scripts/gssSQLConn.sh --logic-file {PROG_PATH}{target_table}/bin/DM_L03_LoadDM_TMP.hql --var ACT_YM={{{{ ti.xcom_pull(task_ids='get_xcoms' , key='YMS') }}}}\",\n"
				+ "	dag=dag\n"
				+ ")\n"
				+ "\n"
				+ "##################################################\n"
				+ "\n"
				+ "\n"
				+ "check_fail=MsSqlOperator(\n"
				+ "	task_id='check_fail',\n"
				+ "	mssql_conn_id='MSAPPS',\n"
				+ "	trigger_rule='one_failed',\n"
				+ "	sql=query_fail,\n"
				+ "	dag=dag,\n"
				+ ") \n"
				+ "\n"
				+ "end_status=MsSqlOperator(\n"
				+ "	task_id='end_status',\n"
				+ "	mssql_conn_id='MSAPPS',\n"
				+ "	trigger_rule='all_success',\n"
				+ "	sql=query_end,\n"
				+ "	dag=dag,\n"
				+ ") \n"
				+ "\n"
				+ "######### 執行順序需調整(T與L系列)\n"
				+ "check_status >> get_xcoms >> condition_to_go >> BEFORE_C01\n"
				+ "BEFORE_C01 >> DM_T01 >> BACKUP_DM\n"
				+ "BACKUP_DM >> TRUNCATE_DM >> DM_L02_LoadDM >> DM_L03_LoadDM_TMP >> [check_fail, end_status]\n"
				+ "";
			
		return rs;
	}

}
